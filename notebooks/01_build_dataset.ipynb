{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928b0322",
   "metadata": {},
   "source": [
    "# 1. Dataset Construction and Preprocessing\n",
    "\n",
    "This notebook constructs the master HAR dataset from raw  .dat files, performing initial cleaning and organization for downstream analysis.\n",
    "\n",
    "- **Data Source:** Raw sensor data from 9 subjects performing various activities.\n",
    "- **Processing Steps:** Parse individual subject files, remove transient activities, ensure data types, and merge into a master CSV.\n",
    "- **Outputs:** Per-subject CSVs, master dataset, schema JSON, and processing report.\n",
    "- **Purpose:** Prepare clean, structured data for exploratory data analysis and modeling in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw exists: True Processed exists: True\n"
     ]
    }
   ],
   "source": [
    "# Imports & path setup\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RAW_DIR = Path('C:\\\\Users\\\\ASUS\\\\Desktop\\\\AAAA\\\\final_project\\\\data\\\\raw')\n",
    "PROCESSED_DIR = Path('C:\\\\Users\\\\ASUS\\\\Desktop\\\\AAAA\\\\final_project\\\\data\\\\processed')\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Raw exists:', RAW_DIR.exists(), 'Processed exists:', PROCESSED_DIR.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names & activity mapping\n",
    "# Define the expected column structure for PAMAP2 data files\n",
    "def get_column_names():\n",
    "    cols = ['timestamp','activity_id','heart_rate']\n",
    "    sensors = ['hand','chest','ankle']\n",
    "    imu_cols = [\n",
    "        'imu_temp',\n",
    "        'acc_x_16g','acc_y_16g','acc_z_16g',\n",
    "        'acc_x_6g','acc_y_6g','acc_z_6g',\n",
    "        'gyro_x','gyro_y','gyro_z',\n",
    "        'mag_x','mag_y','mag_z',\n",
    "        'ori_w','ori_x','ori_y','ori_z'\n",
    "    ]\n",
    "    for s in sensors:\n",
    "        for c in imu_cols:\n",
    "            cols.append(f'{s}_{c}')\n",
    "    assert len(cols) == 54, f'Expected 54 columns, got {len(cols)}'\n",
    "    return cols\n",
    "\n",
    "# Mapping of activity IDs to descriptive names\n",
    "ACTIVITY_MAP = {\n",
    "    0: 'Other (transient)',\n",
    "    1: 'Lying',\n",
    "    2: 'Sitting',\n",
    "    3: 'Standing',\n",
    "    4: 'Walking',\n",
    "    5: 'Running',\n",
    "    6: 'Cycling',\n",
    "    7: 'Nordic walking',\n",
    "    9: 'Watching TV',\n",
    "    10: 'Computer work',\n",
    "    11: 'Car driving',\n",
    "    12: 'Ascending stairs',\n",
    "    13: 'Descending stairs',\n",
    "    16: 'Vacuum cleaning',\n",
    "    17: 'Ironing',\n",
    "    18: 'Folding laundry',\n",
    "    19: 'House cleaning',\n",
    "    20: 'Playing soccer',\n",
    "    24: 'Rope jumping'\n",
    "}\n",
    "TRANSIENT_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15019cce",
   "metadata": {},
   "source": [
    "## 1.2 Data Schema and Activity Mapping\n",
    "\n",
    "This section defines the column structure for PAMAP2 sensor data and maps activity IDs to human-readable labels. It includes IMU sensors (accelerometer, gyroscope, magnetometer, orientation) from hand, chest, and ankle, plus heart rate and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29710866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser for a single subject .dat file\n",
    "def parse_subject_dat(file_path: Path) -> pd.DataFrame:\n",
    "    # Read the .dat file with space-separated values, no header\n",
    "    df = pd.read_csv(file_path, sep=r'\\s+', header=None, engine='python')\n",
    "    df.columns = get_column_names()\n",
    "\n",
    "    # Extract subject ID from filename (e.g., subject101.dat -> 101)\n",
    "    m = re.search(r'subject(\\d+)', file_path.name)\n",
    "    subject_id = int(m.group(1)) if m else None\n",
    "    df['subject_id'] = subject_id\n",
    "\n",
    "    # Remove transient / unlabeled activities (activity_id == 0)\n",
    "    df = df[df['activity_id'] != TRANSIENT_ID].copy()\n",
    "\n",
    "    # Ensure numeric types for sensor columns, coerce errors to NaN\n",
    "    num_cols = [c for c in df.columns if c not in ['activity_id','subject_id']]\n",
    "    df[num_cols] = df[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Sort by timestamp to ensure chronological order\n",
    "    df = df.sort_values('timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d8ead",
   "metadata": {},
   "source": [
    "## 1.3 Subject Data Parsing\n",
    "\n",
    "This section defines a function to parse individual subject .dat files into clean DataFrames. It handles column assignment, subject ID extraction, transient activity removal, data type conversion, and timestamp sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86005116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_101.csv rows: 249957\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_102.csv rows: 263349\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_103.csv rows: 174338\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_104.csv rows: 231421\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_105.csv rows: 272442\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_106.csv rows: 250096\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_107.csv rows: 232776\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_108.csv rows: 262102\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\subject_109.csv rows: 6391\n",
      "Wrote C:\\Users\\ASUS\\Desktop\\AAAA\\final_project\\data\\processed\\dataset_master.csv rows: 1942872\n"
     ]
    }
   ],
   "source": [
    "# Process all raw files and write per-subject + master CSV\n",
    "def process_all_raw() -> pd.DataFrame | None:\n",
    "    # Find all subject .dat files in the raw directory\n",
    "    files = sorted(RAW_DIR.glob('subject*.dat'))\n",
    "    if not files:\n",
    "        print('No .dat files found in', RAW_DIR)\n",
    "        return None\n",
    "\n",
    "    per_subject = []\n",
    "    for f in files:\n",
    "        # Parse each subject's data\n",
    "        df = parse_subject_dat(f)\n",
    "        # Extract subject ID for output filename\n",
    "        sid = int(re.search(r'subject(\\d+)', f.name).group(1))\n",
    "        out_path = PROCESSED_DIR / f'subject_{sid}.csv'\n",
    "        # Save individual subject CSV\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print('Wrote', out_path, 'rows:', len(df))\n",
    "        per_subject.append(df)\n",
    "\n",
    "    # Concatenate all subjects into master dataset\n",
    "    master = pd.concat(per_subject, ignore_index=True)\n",
    "    master_out = PROCESSED_DIR / 'dataset_master.csv'\n",
    "    # Save master CSV\n",
    "    master.to_csv(master_out, index=False)\n",
    "    print('Wrote', master_out, 'rows:', len(master))\n",
    "    return master\n",
    "\n",
    "# Execute the processing\n",
    "master_df = process_all_raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae38892",
   "metadata": {},
   "source": [
    "## 1.4 Batch Processing and Master Dataset Creation\n",
    "\n",
    "This section processes all raw .dat files in the raw directory, parses each subject's data, saves individual CSVs, and concatenates them into a master dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved schema.json and processing_report.md\n"
     ]
    }
   ],
   "source": [
    "# Save schema.json and a simple processing report\n",
    "def save_schema_and_report(df: pd.DataFrame) -> None:\n",
    "    # Create schema dictionary with dataset metadata\n",
    "    schema = {\n",
    "        'target': 'activity_id',\n",
    "        'subject_col': 'subject_id',\n",
    "        'timestamp_col': 'timestamp',\n",
    "        'columns': [{'name': c, 'dtype': str(df[c].dtype)} for c in df.columns],\n",
    "        'activity_map': ACTIVITY_MAP\n",
    "    }\n",
    "    # Save schema as JSON\n",
    "    with open(PROCESSED_DIR / 'schema.json', 'w') as f:\n",
    "        json.dump(schema, f, indent=2)\n",
    "\n",
    "    # Generate processing report\n",
    "    report_lines = []\n",
    "    report_lines.append('# Processing Report')\n",
    "    report_lines.append(f'Total rows: {len(df)}')\n",
    "    report_lines.append('Rows per subject:')\n",
    "    rows_per_subject = df.groupby('subject_id').size().to_dict()\n",
    "    for sid, cnt in sorted(rows_per_subject.items()):\n",
    "        report_lines.append(f'- subject {sid}: {cnt}')\n",
    "\n",
    "    # Report missing values\n",
    "    na_counts = df.isna().sum()\n",
    "    top_na = na_counts[na_counts > 0].sort_values(ascending=False)\n",
    "    report_lines.append('Missing values (non-zero):')\n",
    "    for name, cnt in top_na.items():\n",
    "        report_lines.append(f'- {name}: {cnt}')\n",
    "\n",
    "    # Save report as Markdown\n",
    "    with open(PROCESSED_DIR / 'processing_report.md', 'w') as f:\n",
    "        f.write('\\n'.join(report_lines))\n",
    "    print('Saved schema.json and processing_report.md')\n",
    "\n",
    "# Execute if master_df was created\n",
    "if master_df is not None:\n",
    "    save_schema_and_report(master_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9136c203",
   "metadata": {},
   "source": [
    "## 1.5 Schema and Report Generation\n",
    "\n",
    "This section generates metadata files: a JSON schema describing the dataset structure and an activity mapping, plus a Markdown report summarizing processing statistics like row counts and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9420b",
   "metadata": {},
   "source": [
    "## 1.6 Next Steps\n",
    "\n",
    "- **EDA and Modeling:** Use the merged `dataset_master.csv` for exploratory data analysis and model training in Notebook 2.\n",
    "- **Subject Splits:** Respect the fixed subject-based train/validation/test splits to prevent data leakage.\n",
    "- **Advanced Processing:** Implement within-subject imputations, activity restriction, and window-level feature engineering in subsequent notebooks for robust HAR modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
